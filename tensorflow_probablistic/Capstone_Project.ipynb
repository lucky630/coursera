{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "## Probabilistic generative models\n",
    "### Instructions\n",
    "\n",
    "In this notebook, you will practice working with generative models, using both normalising flow networks and the variational autoencoder algorithm. You will create a synthetic dataset with a normalising flow with randomised parameters. This dataset will then be used to train a variational autoencoder, and you will used the trained model to interpolate between the generated images. You will use concepts from throughout this course, including Distribution objects, probabilistic layers, bijectors, ELBO optimisation and KL divergence regularisers.\n",
    "\n",
    "This project is peer-assessed. Within this notebook you will find instructions in each section for how to complete the project. Pay close attention to the instructions as the peer review will be carried out according to a grading rubric that checks key parts of the project instructions. Feel free to add extra cells into the notebook as required.\n",
    "\n",
    "### How to submit\n",
    "\n",
    "When you have completed the Capstone project notebook, you will submit a pdf of the notebook for peer review. First ensure that the notebook has been fully executed from beginning to end, and all of the cell outputs are visible. This is important, as the grading rubric depends on the reviewer being able to view the outputs of your notebook. Save the notebook as a pdf (File -> Download as -> PDF via LaTeX). You should then submit this pdf for review.\n",
    "\n",
    "### Let's get started!\n",
    "\n",
    "We'll start by running some imports below. For this project you are free to make further imports throughout the notebook as you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpl = tfp.layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flags overview image](data/example_images.png)\n",
    "\n",
    "For the capstone project, you will create your own image dataset from contour plots of a transformed distribution using a random normalising flow network. You will then use the variational autoencoder algorithm to train generative and inference networks, and synthesise new images by interpolating in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The normalising flow\n",
    "* To construct the image dataset, you will build a normalising flow to transform the 2-D Gaussian random variable $z=(z_1, z_2)$, which has mean $\\mathbf{0}$ and covariance matrix $\\Sigma=\\sigma^2\\mathbf{I}_2$, with $\\sigma=0.3$. \n",
    "* This normalising flow uses bijectors that are parameterised by the following random variables:\n",
    "  * $\\theta \\sim U[0, 2\\pi)$\n",
    "  * $a \\sim N(3, 1)$\n",
    "  \n",
    "The complete normalising flow is given by the following chain of transformations:\n",
    "* $f_1(z) = (z_1, z_2 - 2)$,\n",
    "* $f_2(z) = (z_1, \\frac{z_2}{2})$,\n",
    "* $f_3(z) = (z_1, z_2 + az_1^2)$,\n",
    "* $f_4(z) = Rz$, where $R$ is a rotation matrix with angle $\\theta$,\n",
    "* $f_5(z) = \\tanh(z)$, where the $\\tanh$ function is applied elementwise.\n",
    "\n",
    "The transformed random variable $x$ is given by $x = f_5(f_4(f_3(f_2(f_1(z)))))$. \n",
    "* You should use or construct bijectors for each of the transformations $f_i$, $i=1,\\ldots, 5$, and use `tfb.Chain` and `tfb.TransformedDistribution` to construct the final transformed distribution. \n",
    "* Ensure to implement the `log_det_jacobian` methods for any subclassed bijectors that you write.\n",
    "* Display a scatter plot of samples from the base distribution.\n",
    "* Display 4 scatter plot images of the transformed distribution from your random normalising flow, using samples of $\\theta$ and $a$. Fix the axes of these 4 plots to the range $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the image dataset\n",
    "* You should now use your random normalising flow to generate an image dataset of contour plots from your random normalising flow network. \n",
    "  * Feel free to get creative and experiment with different architectures to produce different sets of images!\n",
    "* First, display a sample of 4 contour plot images from your normalising flow network using 4 independently sampled sets of parameters. \n",
    "  * You may find the following `get_densities` function useful: this calculates density values for a (batched) Distribution for use in a contour plot. \n",
    "* Your dataset should consist of at least 1000 images, stored in a numpy array of shape `(N, 36, 36, 3)`.  Each image in the dataset should correspond to a contour plot of a transformed distribution from a normalising flow with an independently sampled set of parameters $s, T, S, b$. It will take a few minutes to create the dataset.\n",
    "* As well as the `get_densities` function, the `get_image_array_from_density_values` function will help you to generate the dataset. \n",
    "  * This function creates a numpy array for an image of the contour plot for a given set of density values Z. Feel free to choose your own options for the contour plots.\n",
    "* Display a sample of 20 images from your generated dataset in a figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute transformed distribution densities\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n",
    "inputs = np.transpose(np.stack((X, Y)), [1, 2, 0])\n",
    "\n",
    "def get_densities(transformed_distribution):\n",
    "    \"\"\"\n",
    "    This function takes a (batched) Distribution object as an argument, and returns a numpy \n",
    "    array Z of shape (batch_shape, 100, 100) of density values, that can be used to make a \n",
    "    contour plot with:\n",
    "    plt.contourf(X, Y, Z[b, ...], cmap='hot', levels=100)\n",
    "    where b is an index into the batch shape.\n",
    "    \"\"\"\n",
    "    batch_shape = transformed_distribution.batch_shape\n",
    "    Z = transformed_distribution.prob(np.expand_dims(inputs, 2))\n",
    "    Z = np.transpose(Z, list(range(2, 2+len(batch_shape))) + [0, 1])\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert contour plots to numpy arrays\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "def get_image_array_from_density_values(Z):\n",
    "    \"\"\"\n",
    "    This function takes a numpy array Z of density values of shape (100, 100)\n",
    "    and returns an integer numpy array of shape (36, 36, 3) of pixel values for an image.\n",
    "    \"\"\"\n",
    "    assert Z.shape == (100, 100)\n",
    "    fig = Figure(figsize=(0.5, 0.5))\n",
    "    canvas = FigureCanvas(fig)\n",
    "    ax = fig.gca()\n",
    "    ax.contourf(X, Y, Z, cmap='hot', levels=100)\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout(pad=0)\n",
    "\n",
    "    ax.margins(0)\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    return image_from_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make `tf.data.Dataset` objects\n",
    "* You should now split your dataset to create `tf.data.Dataset` objects for training and validation data. \n",
    "* Using the `map` method, normalise the pixel values so that they lie between 0 and 1.\n",
    "* These Datasets will be used to train a variational autoencoder (VAE). Use the `map` method to return a tuple of input and output Tensors where the image is duplicated as both input and output.\n",
    "* Randomly shuffle the training Dataset.\n",
    "* Batch both datasets with a batch size of 20, setting `drop_remainder=True`.\n",
    "* Print the `element_spec` property for one of the Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the encoder and decoder networks\n",
    "* You should now create the encoder and decoder for the variational autoencoder algorithm.\n",
    "* You should design these networks yourself, subject to the following constraints:\n",
    "   * The encoder and decoder networks should be built using the `Sequential` class.\n",
    "   * The encoder and decoder networks should use probabilistic layers where necessary to represent distributions.\n",
    "   * The prior distribution should be a zero-mean, isotropic Gaussian (identity covariance matrix).\n",
    "   * The encoder network should add the KL divergence loss to the model.\n",
    "* Print the model summary for the encoder and decoder networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the variational autoencoder\n",
    "* You should now train the variational autoencoder. Build the VAE using the `Model` class and the encoder and decoder models. Print the model summary.\n",
    "* Compile the VAE with the negative log likelihood loss and train with the `fit` method, using the training and validation Datasets.\n",
    "* Plot the learning curves for loss vs epoch for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use the encoder and decoder networks\n",
    "* You can now put your encoder and decoder networks into practice!\n",
    "* Randomly sample 1000 images from the dataset, and pass them through the encoder. Display the embeddings in a scatter plot (project to 2 dimensions if the latent space has dimension higher than two).\n",
    "* Randomly sample 4 images from the dataset and for each image, display the original and reconstructed image from the VAE in a figure.\n",
    "  * Use the mean of the output distribution to display the images.\n",
    "* Randomly sample 6 latent variable realisations from the prior distribution, and display the images in a figure.\n",
    "  * Again use the mean of the output distribution to display the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a video of latent space interpolation (not assessed)\n",
    "* Just for fun, you can run the code below to create a video of your decoder's generations, depending on the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create animation\n",
    "\n",
    "import matplotlib.animation as anim\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def get_animation(latent_size, decoder, interpolation_length=500):\n",
    "    assert latent_size >= 2, \"Latent space must be at least 2-dimensional for plotting\"\n",
    "    fig = plt.figure(figsize=(9, 4))  \n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.set_xlim([-3, 3])\n",
    "    ax1.set_ylim([-3, 3])\n",
    "    ax1.set_title(\"Latent space\")\n",
    "    ax1.axes.get_xaxis().set_visible(False)\n",
    "    ax1.axes.get_yaxis().set_visible(False)\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.set_title(\"Data space\")\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    # initializing a line variable \n",
    "    line, = ax1.plot([], [], marker='o')\n",
    "    img2 = ax2.imshow(np.zeros((36, 36, 3)))\n",
    "\n",
    "    freqs = np.random.uniform(low=0.1, high=0.2, size=(latent_size,))\n",
    "    phases = np.random.randn(latent_size)\n",
    "    input_points = np.arange(interpolation_length)\n",
    "    latent_coords = []\n",
    "    for i in range(latent_size):\n",
    "        latent_coords.append(2 * np.sin((freqs[i]*input_points + phases[i])).astype(np.float32))\n",
    "\n",
    "    def animate(i): \n",
    "        z = tf.constant([coord[i] for coord in latent_coords])\n",
    "        img_out = np.squeeze(decoder(z[np.newaxis, ...]).mean().numpy())\n",
    "        line.set_data(z.numpy()[0], z.numpy()[1])\n",
    "        img2.set_data(np.clip(img_out, 0, 1))\n",
    "        return (line, img2)\n",
    "\n",
    "    return anim.FuncAnimation(fig, animate, frames=interpolation_length, \n",
    "                              repeat=False, blit=True, interval=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the animation\n",
    "\n",
    "a = get_animation(latent_size, decoder, interpolation_length=200)\n",
    "HTML(a.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
